{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e8a293",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3acf1dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "Found cached dataset nsmc (/home/ubuntu/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69df5fa2e7b34e15b0bb4232aac034c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "nsmc_dataset = load_dataset('nsmc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1234ac88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 150000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "408195c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '9976970', 'document': '아 더빙.. 진짜 짜증나네요 목소리', 'label': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80d0c153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'document': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['negative', 'positive'], id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c4947c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc_dataset['train'].features['label'].str2int('negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e95f2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc_dataset['train'].features['label'].str2int('positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53bf589d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>6222902</td>\n",
       "      <td>인간이 문제지.. 소는 뭔죄인가..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>8549745</td>\n",
       "      <td>평점이 너무 낮아서...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>9311800</td>\n",
       "      <td>이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>2376369</td>\n",
       "      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>9619869</td>\n",
       "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           document  label\n",
       "0        9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1        3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3        9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
       "...          ...                                                ...    ...\n",
       "149995   6222902                                인간이 문제지.. 소는 뭔죄인가..      0\n",
       "149996   8549745                                      평점이 너무 낮아서...      1\n",
       "149997   9311800                    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n",
       "149998   2376369                        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
       "149999   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
       "\n",
       "[150000 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc_df = nsmc_dataset['train'].to_pandas()\n",
    "nsmc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "775feb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75173</td>\n",
       "      <td>75173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74827</td>\n",
       "      <td>74827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  document\n",
       "label                 \n",
       "0      75173     75173\n",
       "1      74827     74827"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc_df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "971c5875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    150000.000000\n",
       "mean         35.203353\n",
       "std          29.532097\n",
       "min           0.000000\n",
       "25%          16.000000\n",
       "50%          27.000000\n",
       "75%          42.000000\n",
       "max         146.000000\n",
       "Name: review_length, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc_df['review_length'] = nsmc_df['document'].str.len()\n",
    "nsmc_df.review_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f5754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ee52643",
   "metadata": {},
   "source": [
    "### preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d631424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0acf7a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['청', '##춘', '영화', '##의', '최고', '##봉', '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.tokenize('청춘 영화의 최고봉.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "561d2395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 9751, 97707, 42428, 10459, 83491, 118989, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok('청춘 영화의 최고봉.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65352cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 9751, 97707, 42428, 10459, 83491, 118989, 119, 102], [101, 9751, 97707, 102, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok(['청춘 영화의 최고봉.', '청춘'], padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c412c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98129d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(data):\n",
    "    return tok(data['document'], max_length=32, padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eb99215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-aa20a6ab6f8df9c5.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-19926ed592d2f1bc.arrow\n"
     ]
    }
   ],
   "source": [
    "nsmc_dataset_tokenized = nsmc_dataset.map(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8c11655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '9976970',\n",
       " 'document': '아 더빙.. 진짜 짜증나네요 목소리',\n",
       " 'label': 0,\n",
       " 'input_ids': [101,\n",
       "  9519,\n",
       "  9074,\n",
       "  119005,\n",
       "  119,\n",
       "  119,\n",
       "  9708,\n",
       "  119235,\n",
       "  9715,\n",
       "  119230,\n",
       "  16439,\n",
       "  77884,\n",
       "  48549,\n",
       "  9284,\n",
       "  22333,\n",
       "  12692,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc_dataset_tokenized['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1787329b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a3cf8",
   "metadata": {},
   "source": [
    "### model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fc54f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-multilingual-cased',\n",
    "                                                          num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5e0d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 2\n",
    "learning_rate = 2e-7\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e940b64",
   "metadata": {},
   "source": [
    "## train-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28c1c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d6ea5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ds = nsmc_dataset_tokenized['train'].remove_columns(['id', 'document'])\n",
    "tr_ds.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aab5dbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7faded031c10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_dl = DataLoader(tr_ds, batch_size=batch_size)\n",
    "tr_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79788728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7faded03aa60>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds = nsmc_dataset_tokenized['test'].remove_columns(['id', 'document'])\n",
    "val_ds.set_format(type='torch')\n",
    "val_dl = DataLoader(tr_ds, batch_size=batch_size)\n",
    "val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "695fe49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "         0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "         0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "         0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "         1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "         0, 0, 1, 1, 0, 1, 0, 1]),\n",
       " 'input_ids': tensor([[  101,  9519,  9074,  ...,     0,     0,     0],\n",
       "         [  101,   100,   119,  ..., 16439,   102,     0],\n",
       "         [  101,   100,   102,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  9358, 12508,  ...,     0,     0,     0],\n",
       "         [  101,  9519, 25503,  ...,     0,     0,     0],\n",
       "         [  101, 10150, 10954,  ...,  9568, 12310,   102]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(tr_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cf5642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 1172/1172 [04:09<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  308.22439920210593\n",
      "train acc:  0.6012066666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 1172/1172 [01:20<00:00, 14.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:  305.9489754946972\n",
      "val acc:  0.6676466666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 1172/1172 [04:09<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  305.3655588325787\n",
      "train acc:  0.67424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 1172/1172 [01:19<00:00, 14.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:  304.07286612084295\n",
      "val acc:  0.6995666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()\n",
    "\n",
    "model.to(device)\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(tr_dl)):\n",
    "        label = batch['label'].to(device)\n",
    "        input_id, token_type_ids, attention_mask = batch['input_ids'].to(device), batch['token_type_ids'].to(device), batch['attention_mask'].to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        pred = model(input_id, token_type_ids, attention_mask)\n",
    "        \n",
    "        loss = criterion(torch.sigmoid(pred.logits.t()[1]), label.float())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        train_acc += acc(pred.logits.argmax(dim=1), label)\n",
    "        \n",
    "#         if (step+1)%100==0:\n",
    "#             print(\"train loss: \", np.mean(train_losses))\n",
    "#             print(\"train acc: \", train_acc/(step*batch_size))\n",
    "            \n",
    "        \n",
    "    print(\"train loss: \", np.mean(train_losses))\n",
    "    print(\"train acc: \", train_acc/len(tr_dl.dataset))\n",
    "    \n",
    "    val_losses = []\n",
    "    val_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    for step, batch in enumerate(tqdm(val_dl)):\n",
    "        label = batch['label'].to(device)\n",
    "        input_id, token_type_ids, attention_mask = batch['input_ids'].to(device), batch['token_type_ids'].to(device), batch['attention_mask'].to(device)\n",
    "        \n",
    "        pred = model(input_id, token_type_ids, attention_mask)\n",
    "        loss = criterion(torch.sigmoid(pred.logits.t()[1]), label.float())\n",
    "        \n",
    "        val_losses.append(loss.item())\n",
    "        val_acc += acc(pred.logits.argmax(dim=1), label)\n",
    "        \n",
    "\n",
    "    print(\"val loss: \", np.mean(val_losses))\n",
    "    print(\"val acc: \", val_acc/len(val_dl.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14e461e",
   "metadata": {},
   "source": [
    "## train- trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69e62918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "logging_steps = len(nsmc_dataset['train']) // batch_size\n",
    "output_dir = 'trainer_test'\n",
    "\n",
    "training_args = TrainingArguments(output_dir=output_dir,\n",
    "                                 num_train_epochs=num_train_epochs,\n",
    "                                 learning_rate = learning_rate,\n",
    "                                 per_device_train_batch_size=batch_size,\n",
    "                                 per_device_eval_batch_size=batch_size,\n",
    "                                 evaluation_strategy='epoch',\n",
    "                                 logging_steps=logging_steps,\n",
    "                                 fp16=True,\n",
    "                                 push_to_hub=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa451dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "951619fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-multilingual-cased',\n",
    "                                                          num_labels=2)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                 args=training_args,\n",
    "                 compute_metrics=compute_metrics,\n",
    "                 train_dataset=nsmc_dataset_tokenized['train'],\n",
    "                 eval_dataset=nsmc_dataset_tokenized['test'],\n",
    "                 tokenizer=tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2d19a2",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "382459ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2344' max='2344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2344/2344 04:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.668300</td>\n",
       "      <td>0.623203</td>\n",
       "      <td>0.671900</td>\n",
       "      <td>0.657108</td>\n",
       "      <td>0.693383</td>\n",
       "      <td>0.624439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.609800</td>\n",
       "      <td>0.591379</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.678308</td>\n",
       "      <td>0.711451</td>\n",
       "      <td>0.648115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2344, training_loss=0.6390136078050925, metrics={'train_runtime': 294.1886, 'train_samples_per_second': 1019.754, 'train_steps_per_second': 7.968, 'total_flos': 4933332288000000.0, 'train_loss': 0.6390136078050925, 'epoch': 2.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c3fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f13c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
