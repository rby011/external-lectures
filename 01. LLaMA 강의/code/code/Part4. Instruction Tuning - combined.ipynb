{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c720dd14",
   "metadata": {},
   "source": [
    "# load pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a844eb0",
   "metadata": {},
   "source": [
    "1. Misra, Rishabh. \"News Category Dataset.\" arXiv preprint arXiv:2209.11429 (2022).\n",
    "2. Misra, Rishabh and Jigyasa Grover. \"Sculpting Data for ML: The first act of Machine Learning.\" ISBN 9798585463570 (2021).\n",
    "\n",
    "ishabhmisra.github.io/publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "271860d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3811320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"daily_tokenizer_0612\")\n",
    "model = LlamaForCausalLM.from_pretrained('daily_llama_0612')\n",
    "model.to(device)\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f08f21",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34a0cc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/GonzaloA___parquet/GonzaloA--fake_news-1fe2b42e1fa111c8/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b755206454747b99d8ce2d0d729f089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'title', 'text', 'label'],\n",
       "        num_rows: 24353\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Unnamed: 0', 'title', 'text', 'label'],\n",
       "        num_rows: 8117\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'title', 'text', 'label'],\n",
       "        num_rows: 8117\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "data = 'GonzaloA/fake_news'\n",
    "dataset_fake = load_dataset(data)\n",
    "dataset_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d9d5489",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fake = DatasetDict({'train': dataset_fake['train'], 'test': dataset_fake['test']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6829ec81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/heegyu___json/heegyu--news-category-balanced-top10-5f881f7cd497c7a8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c36442140648708c445e34764c6518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = 'heegyu/news-category-balanced-top10'\n",
    "\n",
    "dataset_cate = load_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c817f406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/heegyu___json/heegyu--news-category-balanced-top10-5f881f7cd497c7a8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-31c8659ca0784ee1.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['link', 'headline', 'category', 'short_description', 'authors', 'date'],\n",
       "        num_rows: 29026\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = dataset_cate['train'].to_pandas().category.unique().tolist()\n",
    "categories.sort()\n",
    "categories = categories[:4]\n",
    "\n",
    "dataset_cate = dataset_cate.filter(lambda element: element['category'] in categories)\n",
    "dataset_cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fc9abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "int2label_fake = {0: 'False', 1: 'True'}\n",
    "label2int_fake = {'False': 0, 'True': 1}\n",
    "\n",
    "categories = [x.split(' ')[0].lower() for x in categories]\n",
    "int2label_cate = {i: categories[i] for i in range(len(categories))}\n",
    "label2int_cate = {int2label_cate[key]:key for key in int2label_cate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb8b1b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/heegyu___json/heegyu--news-category-balanced-top10-5f881f7cd497c7a8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-7a735ffebc15a3a9.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['link', 'headline', 'category', 'short_description', 'authors', 'date', 'label'],\n",
       "        num_rows: 26123\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['link', 'headline', 'category', 'short_description', 'authors', 'date', 'label'],\n",
       "        num_rows: 2903\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_label(element):\n",
    "    category = element['category'].split(' ')[0].lower()\n",
    "    return {'label': label2int_cate[category], 'category': category}\n",
    "\n",
    "dataset_cate = dataset_cate.map(gen_label)\n",
    "dataset_cate = dataset_cate['train'].train_test_split(test_size=0.1)\n",
    "dataset_cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "090d3ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/GonzaloA___parquet/GonzaloA--fake_news-1fe2b42e1fa111c8/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-f692b06b8d774d48.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "from datasets import concatenate_datasets\n",
    "import random\n",
    "\n",
    "prompt_format1_fake = \"\"\"Determine if the given article is fake. article: %s  answer: %s\"\"\"\n",
    "prompt_format2_fake = \"\"\"Is this article fake? article: %s answer: %s\"\"\"\n",
    "prompt_format3_fake = \"\"\"Return True if the given article is fake. article: %s answer: %s\"\"\"\n",
    "\n",
    "prompts_fake = [prompt_format1_fake, prompt_format2_fake, prompt_format3_fake]\n",
    "def gen_prompt_fake(element):\n",
    "    prompt_format = prompts_fake[random.randint(0, len(prompts_fake)-1)]\n",
    "    return DatasetDict({'input': prompt_format%(element['title'], int2label_fake[element['label']])})\n",
    "\n",
    "\n",
    "prompt_format1_cate = \"\"\"Given the article, what is the topic of the article? article: %s  answer: %s\"\"\"\n",
    "prompt_format2_cate = \"\"\"Determine the topic of the news article. article: %s answer: %s\"\"\"\n",
    "prompt_format3_cate = \"\"\"What is this article about? business/entertainment/food/healthy/parenting article: %s answer: %s\"\"\"\n",
    "\n",
    "prompts_cate = [prompt_format1_cate, prompt_format2_cate, prompt_format3_cate]\n",
    "\n",
    "def gen_prompt_cate(element):\n",
    "    prompt_format = prompts_cate[random.randint(0, len(prompts_cate)-1)]\n",
    "    return DatasetDict({'input': prompt_format%(element['headline'], int2label_cate[element['label']])})\n",
    "\n",
    "\n",
    "train_fake = dataset_fake['train'].map(gen_prompt_fake, remove_columns=dataset_fake['train'].column_names)\n",
    "train_cate = dataset_cate['train'].map(gen_prompt_cate, remove_columns=dataset_cate['train'].column_names)\n",
    "\n",
    "train_dataset = concatenate_datasets([train_fake, train_cate]).shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0545973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50476 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 50476\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    outputs = tokenizer(\n",
    "        element['input'],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=True,\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    return {\"input_ids\": outputs[\"input_ids\"]}\n",
    "\n",
    "\n",
    "context_length=128\n",
    "tokenized_datasets = train_dataset.map(\n",
    "    tokenize, batched=True, remove_columns=train_dataset.column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1814d5f",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19c900ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 77])\n",
      "attention_mask shape: torch.Size([5, 77])\n",
      "labels shape: torch.Size([5, 77])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "out = data_collator([tokenized_datasets[i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3ea1de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"combined_instruct_llama\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=1_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f96ff49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1577' max='1577' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1577/1577 05:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1577, training_loss=2.992062101200856, metrics={'train_runtime': 309.5588, 'train_samples_per_second': 163.058, 'train_steps_per_second': 5.094, 'total_flos': 976406806204416.0, 'train_loss': 2.992062101200856, 'epoch': 1.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c67738",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7469cb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/GonzaloA___parquet/GonzaloA--fake_news-1fe2b42e1fa111c8/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-deca1f9ef296179e.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/GonzaloA___parquet/GonzaloA--fake_news-1fe2b42e1fa111c8/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-61bd816e220d3827.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"daily_tokenizer_0612\", padding_side='left')\n",
    "\n",
    "\n",
    "prompt_format1 = \"\"\"Determine if the given article is fake. article: %s  answer:\"\"\"\n",
    "prompt_format2 = \"\"\"Is this article fake? article: %s answer:\"\"\"\n",
    "prompt_format3 = \"\"\"Return True if the given article is fake. article: %s answer:\"\"\"\n",
    "\n",
    "prompts = [prompt_format1, prompt_format2, prompt_format3]\n",
    "\n",
    "def gen_valid_prompt_fake(element):\n",
    "    prompt_format = prompts[random.randint(0, len(prompts)-1)]\n",
    "    return DatasetDict({'input': prompt_format%(element['title'])})\n",
    "\n",
    "\n",
    "valid_dataset = dataset_fake['test'].select(range(100)).map(gen_valid_prompt_fake)\n",
    "\n",
    "context_length=128\n",
    "valid_dataset = valid_dataset.map(\n",
    "    tokenize, batched=True, remove_columns=['text', 'input', 'Unnamed: 0', 'title']\n",
    ")\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c962ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=4\n",
    "val_ds = valid_dataset\n",
    "val_ds.set_format(type='torch')\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "291f5e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:07<00:00,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc:  0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def acc(pred,label):\n",
    "    return torch.sum(torch.tensor(pred) == label.squeeze()).item()\n",
    "\n",
    "model.eval()\n",
    "val_losses = []\n",
    "val_acc = 0\n",
    "\n",
    "for step, batch in enumerate(tqdm(val_dl)):\n",
    "    label = batch['label']\n",
    "    input_id= batch['input_ids'].to(device)\n",
    "\n",
    "    pred = model.generate(input_id, max_length=128)\n",
    "    decoded_pred = tokenizer.batch_decode(pred, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    decoded_pred = [re.findall(\"answer: (True|False)\", x)[0] if re.findall(\"answer: (True|False)\", x) else 'none' for x in decoded_pred]\n",
    "    decoded_pred = [label2int_fake[x] if x in label2int_fake else -1 for x in decoded_pred]\n",
    "\n",
    "    val_acc += acc(decoded_pred, label)\n",
    "    \n",
    "    if step == 100:\n",
    "        break\n",
    "\n",
    "print(\"val acc: \", val_acc/((step+1)*batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13f7df0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2903 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2903 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids'],\n",
       "    num_rows: 2903\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"daily_tokenizer_0612\", padding_side='left')\n",
    "prompt_format1 = \"\"\"Given the article, what is the topic of the article? article: %s  answer:\"\"\"\n",
    "prompt_format2 = \"\"\"Determine the topic of the news article. article: %s answer:\"\"\"\n",
    "prompt_format3 = \"\"\"What is this article about? business/entertainment/food/healthy/parenting article: %s answer:\"\"\"\n",
    "\n",
    "prompts = [prompt_format1, prompt_format2, prompt_format3]\n",
    "\n",
    "def gen_valid_prompt_cate(element):\n",
    "    prompt_format = prompts[random.randint(0, len(prompts)-1)]\n",
    "    return DatasetDict({'input': prompt_format%(element['headline'])})\n",
    "\n",
    "valid_dataset = dataset_cate['test'].map(gen_valid_prompt_cate)\n",
    "\n",
    "context_length=128\n",
    "valid_dataset = valid_dataset.map(\n",
    "    tokenize, batched=True, remove_columns=['link', 'headline', 'category', 'short_description', 'authors', 'date', 'input']\n",
    ")\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6dc5af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=4\n",
    "val_ds = valid_dataset.select(range(100))\n",
    "val_ds.set_format(type='torch')\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9061c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:12<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc:  0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "val_losses = []\n",
    "val_acc = 0\n",
    "\n",
    "for step, batch in enumerate(tqdm(val_dl)):\n",
    "    device='cuda:0'\n",
    "    label = batch['label']\n",
    "    input_id= batch['input_ids'].to(device)\n",
    "\n",
    "    pred = model.generate(input_id, max_length=150)\n",
    "    decoded_pred = tokenizer.batch_decode(pred, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    decoded_pred = [re.findall(\"answer: ([a-z]+)\", x)[0] if re.findall(\"answer: ([a-z]+)\", x) else 'none' for x in decoded_pred]\n",
    "    decoded_pred = [label2int_cate[x] if x in label2int_cate else -1 for x in decoded_pred]\n",
    "\n",
    "    val_acc += acc(decoded_pred, label)\n",
    "\n",
    "print(\"val acc: \", val_acc/len(val_dl.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35528ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('llama_combined_0618')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bedc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
