{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae045773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dbc629",
   "metadata": {},
   "source": [
    "## load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e580b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(50000, 512, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
       "          (down_proj): Linear(in_features=1376, out_features=512, bias=False)\n",
       "          (up_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('daily_tokenizer_0612')\n",
    "model = LlamaForCausalLM.from_pretrained('daily_llama_0612')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ccd45e",
   "metadata": {},
   "source": [
    "## get peft model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bb409b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeb5e3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<TaskType.SEQ_CLS: 'SEQ_CLS'>,\n",
       " <TaskType.SEQ_2_SEQ_LM: 'SEQ_2_SEQ_LM'>,\n",
       " <TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
       " <TaskType.TOKEN_CLS: 'TOKEN_CLS'>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(TaskType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6996868e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(50000, 512, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=512, out_features=512, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=512, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=512, out_features=512, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=512, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
       "              (down_proj): Linear(in_features=1376, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=50000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM,\n",
    "                        inference_mode=False,\n",
    "                        r=32,\n",
    "                        lora_alpha=32,\n",
    "                        lora_dropout=0.1)\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f04df95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 262144 || all params: 64115200 || trainable%: 0.4088640447195049\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f39dbd",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a34979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/heegyu___json/heegyu--news-category-balanced-top10-5f881f7cd497c7a8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f08c0b3a11a4c50a940c57dd4c7a8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_cate = load_dataset('heegyu/news-category-balanced-top10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6971ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/heegyu___json/heegyu--news-category-balanced-top10-5f881f7cd497c7a8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-31c8659ca0784ee1.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['link', 'headline', 'category', 'short_description', 'authors', 'date'],\n",
       "        num_rows: 29026\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = dataset_cate['train'].to_pandas().category.unique().tolist()\n",
    "categories.sort()\n",
    "categories = categories[:4]\n",
    "\n",
    "dataset_cate = dataset_cate.filter(lambda element: element['category'] in categories)\n",
    "dataset_cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b98b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [x.split(' ')[0].lower() for x in categories]\n",
    "int2label_cate = {i: categories[i] for i in range(len(categories))}\n",
    "label2int_cate = {int2label_cate[key]:key for key in int2label_cate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4592b46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/heegyu___json/heegyu--news-category-balanced-top10-5f881f7cd497c7a8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-7a735ffebc15a3a9.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['link', 'headline', 'category', 'short_description', 'authors', 'date', 'label'],\n",
       "        num_rows: 26123\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['link', 'headline', 'category', 'short_description', 'authors', 'date', 'label'],\n",
       "        num_rows: 2903\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_label(element):\n",
    "    category = element['category'].split(' ')[0].lower()\n",
    "    return {'label': label2int_cate[category], 'category': category}\n",
    "\n",
    "dataset_cate = dataset_cate.map(gen_label)\n",
    "dataset_cate = dataset_cate['train'].train_test_split(test_size=0.1)\n",
    "dataset_cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61b4cc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "from datasets import concatenate_datasets\n",
    "import random\n",
    "\n",
    "prompt_format1_cate = \"\"\"Given the article, what is the topic of the article? article: %s  answer: %s\"\"\"\n",
    "prompt_format2_cate = \"\"\"Determine the topic of the news article. article: %s answer: %s\"\"\"\n",
    "prompt_format3_cate = \"\"\"What is this article about? business/entertainment/food/healthy/parenting article: %s answer: %s\"\"\"\n",
    "\n",
    "prompts_cate = [prompt_format1_cate, prompt_format2_cate, prompt_format3_cate]\n",
    "\n",
    "def gen_prompt_cate(element):\n",
    "    prompt_format = prompts_cate[random.randint(0, len(prompts_cate)-1)]\n",
    "    return DatasetDict({'input': prompt_format%(element['headline'], int2label_cate[element['label']])})\n",
    "\n",
    "train_cate = dataset_cate['train'].map(gen_prompt_cate, remove_columns=dataset_cate['train'].column_names)\n",
    "train_dataset = train_cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83c00a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 26123\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    outputs = tokenizer(\n",
    "        element['input'],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=True,\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    return {\"input_ids\": outputs[\"input_ids\"]}\n",
    "\n",
    "\n",
    "context_length=128\n",
    "tokenized_datasets = train_dataset.map(\n",
    "    tokenize, batched=True, remove_columns=train_dataset.column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c84d5",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1066ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 54])\n",
      "attention_mask shape: torch.Size([5, 54])\n",
      "labels shape: torch.Size([5, 54])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "out = data_collator([tokenized_datasets[i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d95d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"peft_llama\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=1_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f684141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='816' max='816' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [816/816 01:53, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=816, training_loss=4.009374880323223, metrics={'train_runtime': 114.0944, 'train_samples_per_second': 228.96, 'train_steps_per_second': 7.152, 'total_flos': 362447135539200.0, 'train_loss': 4.009374880323223, 'epoch': 1.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf946d",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e779c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What is the topic of the collowing article? article: Boeing CEO says he assured Trump about Air Force One costs answer: business/entering the 'Socusing'  answer: business/\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load.eval()\n",
    "\n",
    "prompt = \"\"\"\\\n",
    "What is the topic of the collowing article? article: Boeing CEO says he assured Trump about Air Force One costs answer:\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "\n",
    "# Generate\n",
    "generate_ids = model_load.generate(input_ids=inputs.input_ids, max_length=40)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True,\n",
    "                    clean_up_tokenization_spaces=False)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e142253e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2903 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2903 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids'],\n",
       "    num_rows: 2903\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"daily_tokenizer_0612\", padding_side='left')\n",
    "prompt_format1 = \"\"\"Given the article, what is the topic of the article? article: %s  answer:\"\"\"\n",
    "prompt_format2 = \"\"\"Determine the topic of the news article. article: %s answer:\"\"\"\n",
    "prompt_format3 = \"\"\"What is this article about? business/entertainment/food/healthy/parenting article: %s answer:\"\"\"\n",
    "\n",
    "prompts = [prompt_format1, prompt_format2, prompt_format3]\n",
    "\n",
    "def gen_valid_prompt_cate(element):\n",
    "    prompt_format = prompts[random.randint(0, len(prompts)-1)]\n",
    "    return DatasetDict({'input': prompt_format%(element['headline'])})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "valid_dataset = dataset_cate['test'].map(gen_valid_prompt_cate)\n",
    "\n",
    "context_length=128\n",
    "valid_dataset = valid_dataset.map(\n",
    "    tokenize, batched=True, remove_columns=['link', 'headline', 'category', 'short_description', 'authors', 'date', 'input']\n",
    ")\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54e8cdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=4\n",
    "val_ds = valid_dataset.select(range(100))\n",
    "val_ds.set_format(type='torch')\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e833077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:02<00:00,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc:  0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "val_acc = 0\n",
    "\n",
    "for step, batch in enumerate(tqdm(val_dl)):\n",
    "    label = batch['label']\n",
    "    \n",
    "    input_id = batch['input_ids'].to(device)\n",
    "    \n",
    "    pred = model.generate(input_ids=input_id, max_length=70)\n",
    "    decoded_pred = tokenizer.batch_decode(pred, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    decoded_pred = [re.findall(\"answer: ([a-z]+)\", x)[0] if re.findall(\"answer: ([a-z]+)\", x) else 'none' for x in decoded_pred]\n",
    "    decoded_pred = [label2int_cate[x] if x in label2int_cate else -1 for x in decoded_pred]\n",
    "    \n",
    "    val_acc += acc(decoded_pred, label)\n",
    "    \n",
    "print(\"val acc: \", val_acc/len(val_dl.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb71079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('peft_llama_adapter__')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb2ffd1",
   "metadata": {},
   "source": [
    "## model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3eb3ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0055246353149414"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.stat('peft_llama_adapter__/adapter_model.bin').st_size/(1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5528e6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243.59453010559082"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.stat('daily_llama_0612/pytorch_model.bin').st_size/(1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bcec9a",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "501c64fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(50000, 512, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=512, out_features=512, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=512, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=512, out_features=512, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=512, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
       "              (down_proj): Linear(in_features=1376, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=50000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load =  LlamaForCausalLM.from_pretrained('daily_llama_0612')\n",
    "model_load = PeftModel.from_pretrained(model_load, 'peft_llama_adapter')\n",
    "model_load.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31dfbe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import torch\n",
    "\n",
    "def acc(pred,label):\n",
    "    return torch.sum(torch.tensor(pred) == label.squeeze()).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "915942e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:02<00:00,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc:  0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_load.eval()\n",
    "model_load.to(device)\n",
    "\n",
    "val_acc = 0\n",
    "\n",
    "for step, batch in enumerate(tqdm(val_dl)):\n",
    "    label = batch['label']\n",
    "    \n",
    "    input_id = batch['input_ids'].to(device)\n",
    "    \n",
    "    pred = model_load.generate(input_ids=input_id, max_length=70)\n",
    "    decoded_pred = tokenizer.batch_decode(pred, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    decoded_pred = [re.findall(\"answer: ([a-z]+)\", x)[0] if re.findall(\"answer: ([a-z]+)\", x) else 'none' for x in decoded_pred]\n",
    "    decoded_pred = [label2int_cate[x] if x in label2int_cate else -1 for x in decoded_pred]\n",
    "    \n",
    "    val_acc += acc(decoded_pred, label)\n",
    "    \n",
    "print(\"val acc: \", val_acc/len(val_dl.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3cb4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
